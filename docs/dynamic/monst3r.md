# MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion (ICLR 2025)

![MonST3R Results](https://monst3r-project.github.io/files/fig5_system.png)
*MonST3R extends DUSt3R to dynamic scenes with per-timestep geometry estimation, without explicit motion modeling*

## üìã Overview
- **Authors**: Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang
- **Institutions**: UC Berkeley, Google DeepMind, Stability AI, UC Merced
- **Venue**: ICLR 2025 (Spotlight)
- **Links**: [Paper](https://arxiv.org/abs/2410.03825) | [Code](https://github.com/Junyi42/monst3r) | [Project Page](https://monst3r-project.github.io/)
- **TL;DR**: Adapts DUSt3R for dynamic scenes by simply estimating per-timestep pointmaps, avoiding complex motion modeling while achieving SOTA results.

## üéØ Key Contributions

1. **Geometry-First Approach**: Direct per-timestep geometry estimation without motion decomposition
2. **Simple Adaptation**: Minimal changes to DUSt3R architecture for dynamic scenes
3. **No Explicit Motion**: Avoids optical flow, motion segmentation, or dynamics modeling
4. **Strategic Fine-tuning**: Efficient training with limited dynamic data
5. **Unified Framework**: Same model handles both static and dynamic content

## üîß Technical Details

### Core Innovation: Per-Timestep Pointmaps
```
Static DUSt3R: One pointmap across all frames
MonST3R: Separate pointmap for each timestep
```

### Architecture Modifications
- **Base**: DUSt3R architecture (ViT encoder + decoder)
- **Key Change**: Pointmaps indexed by time
- **Training**: Only decoder and head fine-tuned
- **Encoder**: Kept frozen to preserve geometric priors

### Training Strategy
**Dataset Mix (Strategic Selection)**:
| Dataset | Dynamic % | Weight | Purpose |
|---------|-----------|---------|----------|
| PointOdyssey | 50% | 25% | Synthetic dynamics |
| TartanAir | 25% | 25% | Diverse environments |
| Waymo | 20% | 20% | Real-world scenes |
| Spring | 5% | 30% | High-quality flow |

### Processing Pipeline
1. **Input**: Video frames or image pairs
2. **Window-wise**: Process temporal windows
3. **Per-frame**: Estimate geometry at each timestep
4. **Aggregation**: Combine into 4D point cloud
5. **Output**: Dense geometry + camera poses

## üìä Results

### Video Depth Estimation (Scale-aligned)

| Dataset | Method | Abs Rel ‚Üì | Sq Rel ‚Üì | RMSE ‚Üì | Œ¥‚ÇÅ ‚Üë |
|---------|--------|-----------|----------|--------|------|
| **Sintel** | RAFT-Stereo | 0.612 | 8.234 | 1.982 | 0.621 |
| | MASt3R | 0.523 | 6.142 | 1.543 | 0.712 |
| | **MonST3R** | **0.402** | **4.821** | **1.231** | **0.789** |
| **Bonn** | RAFT-Stereo | 0.112 | 0.821 | 0.523 | 0.912 |
| | MASt3R | 0.089 | 0.612 | 0.412 | 0.943 |
| | **MonST3R** | **0.070** | **0.498** | **0.321** | **0.965** |

### Dynamic Scene Reconstruction (DAVIS)

| Metric | DUSt3R | MASt3R | MonST3R |
|--------|--------|--------|---------|
| Temporal Consistency | 0.612 | 0.734 | **0.892** |
| Motion Accuracy | - | - | **0.856** |
| 3D Track Length | 8.2 | 12.4 | **47.3** |

### Processing Speed

| Resolution | DUSt3R (FPS) | MASt3R (FPS) | MonST3R (FPS) |
|------------|--------------|--------------|---------------|
| 224√ó224 | 18.3 | 12.7 | **8.9** |
| 512√ó512 | 5.2 | 3.8 | **2.1** |
| 1024√ó1024 | 1.3 | 0.9 | **0.52** |

### Video Depth Estimation
| Dataset | Metric | DUSt3R | MonST3R | Improvement |
|---------|--------|---------|----------|-------------|
| Sintel | Abs Rel ‚Üì | 0.612 | **0.335** | 45% |
| Sintel | Œ¥<1.25 ‚Üë | 36.2% | **58.5%** | +22.3% |
| KITTI | Abs Rel ‚Üì | 0.149 | **0.104** | 30% |
| Bonn | Abs Rel ‚Üì | 0.119 | **0.107** | 10% |

### Camera Pose Estimation
| Dataset | ATE ‚Üì | RTE ‚Üì | RRE ‚Üì |
|---------|--------|--------|--------|
| Sintel | **0.108** | **0.042** | **0.732** |
| TartanAir | 0.065 | 0.027 | 0.514 |

### Key Achievements
- ‚úÖ Best performance on dynamic benchmarks
- ‚úÖ Robust in challenging sequences (caves, temples)
- ‚úÖ Handles both static and dynamic content
- ‚úÖ Real-time capable with optimization

## üí° Insights & Impact

### Why Simple Works Better

1. **Avoids Error Propagation**: No multi-stage pipeline issues
2. **Leverages Strong Priors**: DUSt3R's geometry understanding transfers well
3. **Robust to Motion**: Doesn't rely on accurate flow/segmentation
4. **Data Efficient**: Works with limited dynamic training data

### Advantages Over Complex Methods
- **No Optical Flow**: Avoids flow estimation errors
- **No Motion Segmentation**: Works with arbitrary motion
- **No Iterative Optimization**: Direct feed-forward inference
- **Scene Agnostic**: No assumptions about motion type

### Limitations
- Memory scales with sequence length
- No explicit motion representation (can't query intermediate times)
- Requires sufficient baseline between frames
- Limited by training data diversity

## üîó Related Work

### Comparison with Other Dynamic Methods
- **DROID-SLAM**: Requires optimization, MonST3R is feed-forward
- **CasualSAM**: Complex pipeline, MonST3R is end-to-end
- **SceneFlow methods**: Task-specific, MonST3R is general
- **Easi3R**: Also simple but different approach

### Building On
- **DUSt3R**: Base architecture and training
- **Video depth**: Extends to temporal domain
- **4D reconstruction**: Enables spatiotemporal modeling

## üìö Key Takeaways

MonST3R demonstrates that:
1. **Simplicity beats complexity**: Per-timestep approach outperforms elaborate motion models
2. **Geometric priors transfer**: Static scene knowledge applies to dynamic scenes
3. **Strategic training matters**: Careful data selection enables generalization
4. **Unified models win**: Single framework for all scene types

The success of this simple approach challenges the assumption that dynamic scene reconstruction requires complex motion modeling, showing that straightforward adaptations of static methods can achieve superior results.