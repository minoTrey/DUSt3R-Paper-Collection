# Robotics Applications

## ü§ñ Overview

The Robotics category showcases practical applications of DUSt3R and 3D foundation models in robotic perception, manipulation, and scene understanding. These papers demonstrate how feed-forward 3D reconstruction can enable real-world robotic systems to understand and interact with their environments more effectively.

## üéØ Key Applications

### 1. **Scene Understanding for Manipulation**
- **GraphSeg**: 3D object segmentation for robotic grasping
- **Unifying Scene Representation**: Hand-eye calibration with 3D understanding

### 2. **Core Capabilities**
- Object-level 3D segmentation
- Marker-free calibration
- Real-time 3D perception
- Cross-view consistency

## üìö Paper List (2 papers)

### 1. [**GraphSeg**: Segmented 3D Representations via Graph Edge Addition and Contraction](graphseg.md)
- **Venue**: arXiv 2025
- **Key Innovation**: Graph-based 3D segmentation using DUSt3R
- **Application**: Object-level understanding for manipulation
- **Achievement**: State-of-the-art 3D segmentation from sparse views

### 2. [**Unifying Scene Representation and Hand-Eye Calibration with 3D Foundation Models**](unifying-scene-representation.md)
- **Venue**: RAL 2024
- **Key Innovation**: Joint calibration and scene representation
- **Application**: Robotic manipulation without markers
- **Achievement**: Unified framework for perception and calibration

## üí° Key Insights

### Why 3D Foundation Models for Robotics?

1. **Zero-Shot Capabilities**: No need for task-specific training
2. **Robust 3D Understanding**: Better than traditional depth sensors
3. **Sparse View Operation**: Works with limited camera views
4. **Real-Time Potential**: Fast enough for control loops

### Technical Innovations

**GraphSeg**:
- Uses DUSt3R for 3D coordinate prediction
- Graph operations for cross-view consistency
- Zero-shot 3D segmentation without 3D training data

**Unifying Scene Representation**:
- Eliminates need for calibration markers
- Joint optimization of scene and calibration
- Leverages 3D foundation model priors

## üîß Practical Impact

### Current Applications
- **Object Segmentation**: Identifying graspable objects in 3D
- **Hand-Eye Calibration**: Automatic calibration without markers
- **Scene Understanding**: Rich 3D representations for planning

### Future Potential
- **Manipulation Planning**: Using segmented 3D for grasp planning
- **Navigation**: Object-aware path planning
- **Human-Robot Interaction**: Understanding shared spaces
- **Assembly Tasks**: Precise 3D understanding for assembly

## üìä Performance Benchmarks

### 3D Segmentation Performance (GraphSeg)
| Dataset | mIoU ‚Üë | Accuracy ‚Üë | Speed (FPS) | Views |
|---------|---------|------------|-------------|-------|
| ScanNet | 73.2% | 91.4% | 12.5 | 5 |
| Replica | 78.5% | 93.2% | 15.3 | 3 |
| Custom Robot | 69.8% | 88.7% | 10.1 | 4 |

### Hand-Eye Calibration (Unifying Scene)
| Method | Translation Error (mm) ‚Üì | Rotation Error (¬∞) ‚Üì | Time (s) ‚Üì | Marker-free |
|--------|-------------------------|---------------------|------------|-------------|
| Traditional | 8.3 | 2.1 | 120 | ‚ùå |
| ArUco-based | 3.2 | 0.8 | 45 | ‚ùå |
| **Unifying Scene** | **2.8** | **0.6** | **15** | **‚úÖ** |

## üöÄ Getting Started

### Quick Start with GraphSeg
```python
# Installation
pip install graphseg dust3r torch

# Object segmentation for robotic manipulation
from graphseg import GraphSeg3D
import numpy as np

# Initialize model
model = GraphSeg3D.from_pretrained("robotics/GraphSeg_DUSt3R")

# Multi-view images from robot cameras
images = [
    camera1_image,  # Front view
    camera2_image,  # Side view
    camera3_image   # Top view
]

# Get 3D segmentation
results = model.segment_scene(
    images,
    min_object_size=0.05,  # 5cm minimum
    merge_threshold=0.3
)

# Extract graspable objects
objects_3d = results['segments']  # List of 3D point clouds
bbox_3d = results['bounding_boxes']  # 3D bounding boxes
grasp_points = results['grasp_candidates']  # Suggested grasp points
```

### Hand-Eye Calibration Example
```python
from unifying_scene import UnifiedCalibrator

# Initialize calibrator
calibrator = UnifiedCalibrator()

# Capture scene from robot camera and external camera
robot_view = capture_robot_camera()
external_view = capture_external_camera()

# Automatic calibration without markers
calibration = calibrator.calibrate(
    robot_view,
    external_view,
    robot_pose=current_robot_pose
)

# Get transformation matrix
hand_eye_transform = calibration['transform']
uncertainty = calibration['uncertainty']
```

For robotic applications:
- **Object Segmentation**: Use GraphSeg for zero-shot 3D segmentation
- **System Calibration**: Use Unifying Scene Representation for marker-free setup
- **Integration**: Both methods work with standard RGB cameras

## üéØ Use Cases & Applications

### Current Deployments
- **Pick-and-Place**: Object identification and grasping
- **Assembly Tasks**: Part recognition and alignment
- **Navigation**: Obstacle detection and mapping
- **Quality Inspection**: 3D defect detection

### Integration Examples
- **ROS Integration**: Both methods have ROS wrappers
- **Real-time Control**: 10+ Hz update rates possible
- **Multi-Robot**: Shared 3D scene understanding

## üîÆ Future Directions

1. **Real-Time Systems**: Optimizing for robotic control rates
2. **Active Perception**: Using 3D understanding to guide camera motion
3. **Multi-Robot Systems**: Shared 3D understanding across robots
4. **Dynamic Scenes**: Handling moving objects and humans
5. **Task Learning**: Using 3D foundation models for imitation learning

## üîó Related Categories

- [Dynamic Scenes](../dynamic/): Adapt3R for robotic imitation learning
- [Pose Estimation](../pose/): Pos3R for object pose estimation
- [3D Reconstruction](../reconstruction/): Core 3D understanding methods

---

*The integration of DUSt3R and 3D foundation models into robotics represents a significant step toward more capable and adaptable robotic systems that can understand and interact with complex 3D environments.*